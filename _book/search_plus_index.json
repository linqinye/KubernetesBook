{"./":{"url":"./","title":"前言","keywords":"","body":"Kubernetes学习笔记 By Dazzle Lin            updated 2019-07-02 20:37:39 "},"Chapter1/跑起来.html":{"url":"Chapter1/跑起来.html","title":"跑起来","keywords":"","body":"kubernetes：https://kubernetes.io/docs/tutorials/kubernetes-basics/ 创建单节点的kubernetes集群 minikube start kubectl get nodes 集群唯一的节点为minikube，当前执行命令的地方并不是minikube，而是通过k8s的命令行工具kubectl远程管理集群 查看集群信息 kubectl cluster-info 部署 kubectl run kubernetes-bootcamp \\ > --image=docker.io/jocatalin/kubernetes-bootcamp:v1 \\ > --port=8080 Deployment：应用 Pod:容器的集合,k8s最小的调度单位，同一个pod中的容器始终被一起调度。通常将紧密相关的一组容器放到一个Pod中，同一个Pod中所有容器共享IP地址和Port空间，也就是说它们处于同一个network namespace中. kubectl get pods:查看当前的Pod 访问应用 默认情况所有Pod只能在集群内访问。 为了从外部访问，需要将容器8080端口映射到节点端口 $ kubectl expose deployment/kubernetes-bootcamp \\ > --type=\"NodePort\" \\ > --port 8080 service/kubernetes-bootcamp exposed 执行命令kubectl get services查看应用被映射到节点的哪个端口 kubectl get services $ curl minikube:30906 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-6c5cfd894b-sn68z | v=1 Scale应用 默认情况下应用只会运行一个副本，可以通过kubectl get deployments查看副本数 增加副本 kubectl scale deployments/kubernetes-bootcamp --replicas=3 通过kubectl get pods，发现pod也增加到3个 通过curl访问应用，可以看到每次请求发送到不同的Pod,3个副本轮询处理，这样就实现了负载均衡 scale down，删除副本 kubectl scale deployments/kubernetes-bootcamp --replicas=2 其中一个副本被删除 滚动更新 升级 将当前v1版本的应用更新为V2 kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 通过kubectl get pods可以观察滚动更新的过程，v1的Pod被逐个停掉且删除，同时启动新的v2Pod 回退 kubectl rollout undo deployments/kubernetes-bootcamp （待续）本地搭建配置minikubernetes集群 minikube start --vm-driver=virtualbox --registry-mirror=https://registry.docker-cn.com 下载virtualbox：https://www.virtualbox.org/wiki/Linux_Downloads rpm -ivh VirtualBox-6.0-6.0.8_130520_el7-1.x86_64.rpm #出现缺失依赖 yum list available | grep 依赖 yum install -y SDL yum install qt yum install qt-x11 yum list available | grep opus yum install -y libXmu.x86_64 yum install -y libXt.x86_64 yum install -y libvpx.x86_64 By Dazzle Lin            updated 2019-07-02 22:09:41 "},"Chapter2/概念.html":{"url":"Chapter2/概念.html","title":"概念","keywords":"","body":"结合docker对应 Cluster 计算、存储和网络资源得集合，k8s利用这些资源运行各种基于容器得应用 Master Cluster的大脑，它的主要职责是调度，即决定将应用放到哪里运行。Master运行Linux系统，可以是物理或虚拟机。为了实现高可用，可以运行多个Master。 Node Node职责是运行容器应用。Node由Master管理，Node负责监控并汇报容器状态。同时根据Master要求管理容器的生命周期。 Pod k8s的最小工作单元，每个Pod包含一个或多个容器。Pod中的容器会作为一个整体被Master调度到一个Node上运行。 引入目的 1.可管理性：有些容器天生就需要紧密联系，一起工作 2.通信和资源共享：Pod中所有容器使用同一个namespace，即相同的IP和port。容器可以共享存储，当k8s挂载volume到Pod,本质是将volume挂载到Pod的每个容器上。 两种使用方式 1.运行单一容器：one-container-per-Pod是最常见模型，将单个容器封装成Pod，k8s管理的也只是Pod而不是容器。 2.运行多个容器 Controller(运行Pod) K8s通常不会直接创建Pod，而是通过Controller来管理Pod。Controller中定义了Pod的部署特性，比如几个副本、在什么Node上运行等。 K8s提供的Controller主要有Deployment、ReplicaSet、DaemonSet、StatefuleSet、Job等 Deployment：可以管理Pod的多个副本，并确保Pod按照期望状态运行 ReplicaSet：实现Pod的多副本管理。使用Deployment会自动创建ReplicaSet，即Deployment借助ReplicaSet来实现Pod的多副本管理，通常不需要直接使用 DaemonSet：通常运行daemon，用于每个Node最多只运行一个Pod副本的场景 StatefuleSet：保证Pod的每个副本在整个生命周期中名称是不变的。 当某个Pod出故障删除重启时，名称会发生变化，同时StatefuleSet会保证副本按照固定顺序启动、更细或删除 Job：用于运行结束或删除的应用。其他Controller中的Pod通常是长期持续运行。 Service（访问Pod） 定义了外界访问一组特定Pod的方式。拥有自己的IP和端口，为Pod提供负载均衡。 Namespace 可以将一个物理的Cluster逻辑上划分成多个虚拟Cluster，每个Cluster就是一个Namespace。不同Namespace的资源完全隔离。 K8s默认创建两个Namespace default：创建资源时如果不指定，将被放到这个里面 kube-system：K8S自己创建的系统资源将放到这里。 By Dazzle Lin            updated 2019-07-02 22:12:42 "},"Chapter3/部署Kubernetes Cluster.html":{"url":"Chapter3/部署Kubernetes Cluster.html","title":"部署Kubernetes Cluster","keywords":"","body":"yum update # 配置源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 安装 yum install -y kubelet kubeadm kubectl kubernetes-cni kubelet：运行在Cluster所有节点上，负责启动Pod和容器 kubeadm：初始化Cluster kubectl：K8s命令行工具，可以部署和管理应用，查看资源，创建、删除和更新各种组件 kubeadm初始化Cluster 修改主机名称 sudo hostnamectl set-hostname 更换yum源** 备份本地yum源 yum install -y wget mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak 获取阿里yum源配置文件 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 更新cache yum makecache 查看 yum -y update 启动kubelet systemctl enable docker && systemctl start docker systemctl enable kubelet && systemctl start kubelet（init时会自动启动） #查看错误日志 journalctl -xefu kubelet journalctl -f -u kubelet.service 关闭swap内存 swapoff -a free -h #也可以如下 vi /etc/fstab #/dev/mapper/centos-swap swap swap defaults 0 0 解决 /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 echo \"1\" >/proc/sys/net/bridge/bridge-nf-call-iptables node \"k8s-master\" not found 去掉，使用默认接口配置 vi /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes Repo baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ gpgcheck=0 enabled=1 更新cache yum makecache 查看 yum -y update 初始化过程会出现连接国外网络超时，解决：** docker pull coredns/coredns:1.3.1 docker pull mirrorgooglecontainers/kube-apiserver:v1.14.2 docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.2 docker pull mirrorgooglecontainers/kube-scheduler:v1.14.2 docker pull mirrorgooglecontainers/kube-proxy:v1.14.2 docker pull mirrorgooglecontainers/etcd:3.3.10 docker pull mirrorgooglecontainers/pause:3.1 docker tag mirrorgooglecontainers/kube-apiserver:v1.14.2 k8s.gcr.io/kube-apiserver:v1.14.2 docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.2 k8s.gcr.io/kube-controller-manager:v1.14.2 docker tag mirrorgooglecontainers/kube-scheduler:v1.14.2 k8s.gcr.io/kube-scheduler:v1.14.2 docker tag mirrorgooglecontainers/kube-proxy:v1.14.2 k8s.gcr.io/kube-proxy:v1.14.2 docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10 docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1 中间出现错误需要重置集群节点状态化 kubeadm reset 开始初始化 kubeadm init --kubernetes-version v1.14.2 kubeadm reset && systemctl start kubelet && kubeadm init --kubernetes-version v1.14.2 --apiserver-advertise-address：指明用Master的哪个Interface与Cluster的其他节点通信。如果Master有多个Interface，建议明确指定，如果不指定，kubeadm会自动选择有默认网关的Interface. --pod-network-cidr：指定Pod网络范围。10.244.0.0/16是因为我们将使用flannel网络方案，必须设置成这个CIDR。 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.129.132:6443 --token ysqdvu.dzn7l4shglpbad8t \\ --discovery-token-ca-cert-hash sha256:d503d057101b3e34f705aff4151d6eb679044a9c6897c2220300443c9368b27f 为用户docker配置kubectl groupadd docker useradd -g docker docker passwd docker 执行普通用户命令报错 docker 不在 sudoers 文件中。此事将被报告。 进去root，打开/etc/sudoers。在root ALL=(ALL:ALL) ALL下面添加个人用户，保存即可。 :wq！ 配置kubectl命令执行权限 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 为了更便捷，启用kubectl命令的自动补全功能 echo \"source > ~/.bashrc 设置master为不可调度 kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule 解除配置，设置master可以被调度运行pod kubectl taint nodes --all node-role.kubernetes.io/master- 安装Node加入集群 在Node主机上执行安装：docker、kubelet、kubectl、kubeadm、kubernetes-cni 同时设置docker和kubelet启动和开机启动 具体代码模仿上面 如果 token 失效，到主节点执行：kubeadm token create 重新生成 《时好时坏，随机应变吧》必须在加入集群之前也进行一次kubeadm init，否则无法启动Kubelet 执行kubeadm join命令加入集群 kubeadm join 192.168.129.132:6443 --token ysqdvu.dzn7l4shglpbad8t \\ --discovery-token-ca-cert-hash sha256:d503d057101b3e34f705aff4151d6eb679044a9c6897c2220300443c9368b27f 安装网络插件 查看官网https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ kubectl apply -f https://git.io/weave-kube-1.6 添加flannel网络 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml #版本信息：quay.io/coreos/flannel:v0.10.0-amd64 kubectl create -f kube-flannel.yml By Dazzle Lin            updated 2019-07-02 22:16:53 "},"Chapter4/kubernetes架构.html":{"url":"Chapter4/kubernetes架构.html","title":"kubernetes架构","keywords":"","body":"Master节点 cluster的大脑，运行着Daemon服务 API Server(kube-apiserver) API Server提供HTTP/HTTPS RESTful API。是kubernetes Cluster的前端接口，各种客户端工具通过它管理Cluster各种资源 Scheduler（kube-scheduler） 负责决定将Pod放在哪个Node上运行。 Controller Manager(kube-controller-manager) 负责管理Cluster各种资源，保证资源处于预期状态 不同controller管理不同的资源 etcd 负责保存Kubernetes Cluster的配置信息和各种资源的状态信息，当数据发生变化，etcd会快速通知相关组件。 Pod网络 Pod要能够相互通信，Kubernetes Cluster必须部署Pod网络 Node节点 Node是Pod运行的地方，运行着Kubernetes组件有kubelet、kube-proxy和Pod网络 kubelet Node的agent，当Scheduler确定Pod在哪个Node上运行，会将Pod具体配置信息发送给该节点的kubelet，kubelet根据信息创建和运行容器，并向Master报告运行状态 kube-proxy service在逻辑上代表了后端的多个Pod,外界通过service访问Pod。 service接收到的请求如何转发Pod是kube proxy负责 每个Node都会运行kube-proxy服务，负责将访问service的TCP/UPD数据流转发到后端的容器，如果存在多副本，kube-proxy实现负载均衡。 Pod网络 Master上也可以运行应用，即Master也是一个Node，因此Master上也有kubelet和kebu-proxy几乎所有的Kubernetes组件本身也运行在Pod里。 kubectl get pod --all-namespaces -o wide Kubernetes的系统组件都被放到kube-system namespace里。 kube-dns组件为Cluster提供DNS服务，在执行kubeadm init时作为附加组件安装 kubelet是唯一没有以容器形式运行的Kubernetes组件，通过Systemd服务运行。 By Dazzle Lin            updated 2019-07-02 22:19:18 "},"Chapter4/部署应用.html":{"url":"Chapter4/部署应用.html","title":"部署应用","keywords":"","body":"部署应用 kubectl run httpd-app --image=httpd --replicas=2 #查看deployment的状态 kubectl get deployment kubectl get pod -o wide 分析 1.kubectl发送部署请求到API Server 2.API Server通知Controller Manager创建一个deployment资源 3.Scheduler执行调度任务，将两个副本Pod分发到Node1和Node2 4.Node上的kubectl在各自节点上创建并运行Pod 补充 应用的配置和当前状态信息保存在etcd中。执行kubectl get pod时API Server会从etcd读取数据 By Dazzle Lin            updated 2019-07-02 22:19:44 "},"Chapter4/运行应用.html":{"url":"Chapter4/运行应用.html","title":"运行应用","keywords":"","body":"运行应用 #了解更详细信息 kubectl describe deployment #查看副本信息 kubectl get replicaset -o wide kubectl describe replicaset #查看副本Pod kubectl get pod kubectl describe pod 总结 1.用户通过Kubectl创建Deployment 2.Deployment创建ReplicaSet 3.ReplicaSet创建Pod 副本对象命名:父对象名字+随机字符串或数字 deployment 命令 VS 配置文件 两种创建资源方式 1.kubectl命令直接创建，在命令行通过参数指定资源的属性 简单、直观、快捷、上手快。适合临时测试或实验 kubectl run nginx-deployment --image=nginx:1.7.9 --replicas=2 2.通过配置文件和kubectl apply创建 配置文件描述了应用最终要达到的状态 配置文件提供了创建资源的模板，能重复部署 可以像管理代码一样管理部署 适合正式、跨环境、规模化部署 kubectl apply -f xxx.yml 资源格式为YAML #当前配置格式的版本 apiVersion: extensions/v1beta1 #要创建的资源类型 kind: Deployment #该资源的元数据 metadata: #必需的元数据项 name: nginx-deployment #该Deployment的规格说明 spec: #副本数量，默认1 replicas: 2 #定义Pod模板，重要 template: #Pod元数据，至少要定义一个label metadata: labels: app: web_server #描述Pod规格，此部分定义Pod中每一个容器的属性，name和image是必须的 spec: containers: - name: nginx image: nginx:1.7.9 删除资源 kubectl delete deployment nginx-deployment kubectl delete -f nginx.yml 伸缩 在线增加或减少Pod副本数 修改配置文件副本数,可控制增删副本数（少增多删） 新建副本被创建并调度node1和node2上 默认配置下K8S不会将Pod调度到Master节点 #设置Master作为Pod kubectl taint node k8s-master node-role.kubernetes.io/master- #恢复Master Only状态 kubectl taint node k8s-master node-role.kubernetes.io/master=\"\":NoSchedule Failover 模拟故障，关闭Node2 halt -h K8S检测到node2不可用，自动调度到node1,维持副本数，node2恢复后，Unknown的Pod会被删除，但不会重新调度。 label控制Pod位置 默认情况下，Scheduler会将Pod调度到所有可用的Node,不过有些时候想要将Pod部署到指定的Node label是Key-value对，各种资源都可以设置label，灵活添加自定义属性 #例：标注node是配置了SSD的节点 kubectl label node k8s-node1 disktype=ssd #查看节点的label kubectl get node --show-labels 指定将Pod部署到k8s-node1，编辑nginx.yml,修改内容如下 #描述Pod规格，此部分定义Pod中每一个容器的属性，name和image是必须的 spec: containers: - name: nginx image: nginx:1.7.9 #指定将此Pod部署到具有label disktype=ssd的Node上 nodeSelector: disktype: ssd 部署Deployment并查看Pod的运行节点，查看副本是否全部运行在node1上 kubectl appl -f nginx.yml kuberctl get pod -o wide 删除label disktype kubectl label node k8s-node1 disktype- #-即删除意思 kubectl get node --show-labels### 此时Pod不会重新部署，除非删除yml中的nodeSelector,且通过kubectl apply重新部署 DaemonSet Deployment部署的副本Pod会分布在各个Node上，每个Node都可能运行好几个副本。 DaemonSet不同之处在于每个Node最多运行一个副本 适用场景 1.在集群的每个节点上运行存储Daemon 2.在每个节点上运行日志收集Daemon 3.在每个节点上运行监控Daemon K8s自己就用DaemonSet运行系统组件 kubectl get daemonset --namespace=kube-system kube-flannel和kube-proxy分别负责在每个节点上运行。 必须指定--namespace=kube-system，表示系统组件，否则返回默认namespace default资源 kube-flannel kube-proxy 无法拿到yaml文件，通过下面命令查看配置 kubectl edit daemonset kube-proxy --namespace=kube-system 运行自己的Daemonset Job 容器按持续运行时间可分为两类： 服务类容器 持续提供服务 ，需要一直运行 工作类容器 一次性任务 kubectl get job #Pod执行完毕后会自动退出，因此需要加上--show-all查看Completed状态的Pod kubectl get pod --show-all #查看Pob标准输出 kubectl logs PodName #查看某个Pod启动日志 kubectl describe pod YAML文件注意点 restartPolicy:Never/OnFailure Job并行性 Yaml添加 #并行数 parallelism:2 #设置总共完成数 completions:6 不指定默认1 定时Job CronJob apiVersion: batch/v2alpha1 kind: CronJob metadata: name: hello spec: schedule: cron表达式 jobTemplate: spec: template: spec: containers: name: hello image: busybox command: [\"echo\",\"hello k8s job\"] restartPolicy: OnFailure 第一次kubectl apply -f cronjob.yml肯定失败： K8S默认没有enable CronJob功能，需要在kube-apiserver加入功能，需要修改kube-apiserver配置文件 /etc/kubernetes/manifests/kube-apiserver.yaml 启动参数中加上--runtime-config=apiVersion=true即可，重启kubelet服务 systemctl restart kubelet.service kubelet会重启kube-apiserver Pod #确认kube-apiserver支持apiVersion kubectl api-versions #查看CronJob状态 kubectl get cronjob #查看job kubectl get jobs By Dazzle Lin            updated 2019-07-02 22:22:14 "},"Chapter4/通过Service访问Pod.html":{"url":"Chapter4/通过Service访问Pod.html","title":"通过Service访问Pod","keywords":"","body":"通过Service访问Pod Pod是脆弱的，应用是健壮的 每个Pod都有自己的IP地址，当Controller用新Pod替换故障Pod，IP会随时变化，客户端找到并访问服务。（K8S通过Service） 创建Service 多个资源可以在一个yaml定义，用---分割 apiVersion: V1 kind: Service metadate： #Service名字 name: httpd-svc #指定所属namespace namespace: kube-public spec: #添加Service类型 type: NodePort selector: run: httpd ports: #将Service的8080映射到Pod的80，使用TCP协议 protocol: TCP #节点上监听的端口 nodePort: 30000 #ClusterIP上监听的端口 port: 8080 #Pod监听的端口 targetPort: 80 kubectl get service kubectl describe service httpd-svc Cluster Ip底层实现 Cluster ip是虚拟Ip,由K8S节点上的iptables规则管理 iptables-save #打印当前节点iptables规则 iptables将访问Service的流量转发到后端Pod,而且使用类似轮询的负载均衡策略 补充：Cluster每个节点都配置相同的iptables规则 DNS访问Service kubectl get deployment --namespace=kube-system #Pod与httpd-svc同属default namespace，可忽略default wget httpd-svc.default:8080 #nslookup查看httpd-svc的DNS信息 nslookup httpd-svc 要访问其他namespace中的Service，必须带上namespace wget httpd-svc.kube-public:8080 kubectl get namespace 外网访问Service 默认ClusterIP ClusterIP Service通过Cluster内部IP对外提供服务，只有Cluster内的节点和Pod能访问 NodePort Service通过Cluster节点的静态端口对外提供服务。内部通过:访问Service LoadBalancer By Dazzle Lin            updated 2019-07-02 22:22:50 "},"Chapter4/Rolling Update.html":{"url":"Chapter4/Rolling Update.html","title":"Rolling Update","keywords":"","body":"Rolling Update 滚动更新是一次只更新一小部分副本，成功后再更新更多副本，最终完成所有副本的更新。 最大好处 零停机，整个更新过程始终有副本运行，保证业务连续性 修改镜像版本，即更新，再次执行kubectl apply 之前的三个Pod会被三个新的Pod替换掉 每次替换Pod数量是可以定制，K8S提供maxSurge和maxUnavailable来精细控制Pod的替换数量 maxSurge 控制滚动更新过程副本总数超过DESIRED的上线。默认25%。roundUp(DESIRED+DESIRED*25%) maxUnavailable 控制滚动更新过程中，不可用的副本占DESIRED最大比例。默认25%，DESIRED-roundDown(DESIRED*25%) maxSurge值越大，初始创建的新副本数量就越多；maxUnavailable值越大，初始销毁的旧副本数量旧越多。 定制maxSurge和maxUnavailable spec: strategy: rollingUpdate: maxSurge: 35% maxUnavailable: 35% 回滚 kubectl apply每次更新应用时，k8s会记录下当前的配置，保存为一个revision（版次），这样就可以回滚到特定revision。 默认配置下，K8S只会保留最近的几个revision，可以在Deployment配置文件中通过revisionHistoryLimit属性增加revision数量。 kubactl apply -f httpd.v1.yml --record --record的作用是将当前命令记录到revision记录中， #查看revision历史记录 kubectl rollout history deployment httpd #回滚到某个版本 kubectl rollout undo deployment httpd --to-revision=1 By Dazzle Lin            updated 2019-07-02 22:23:21 "},"Chapter4/Health Check.html":{"url":"Chapter4/Health Check.html","title":"Health Check","keywords":"","body":"Health Check 自愈的默认实现方法是自动重启发生故障的容器 默认的健康检查 每个容器启动时都会执行一个进程，此进程由Dockerfile的CMD或ENTRYPOINT指定。如果进程退出时返回码非零，则认为容器故障。K8S就会根据restartPolicy重启容器 #模拟容器启动10秒后发生故障 - sleep 10; exit 1 Liveness探测 让用户可以自定义判断容器是否健康的条件，探测失败，K8S就会重启容器 Readiness探测 告诉K8S何时将容器加入Service负载均衡池中，对外提供服务 上面两者配置语法类似，yaml文件如下：Liveness将readiness替换即可 apiVersion: v1 kind: Pod meradata: labels: test: readiness name:readiness spec: restartPolicy: OnFailure containers: - name: readiness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 两者探测区别 1.不特意配置，K8S对两者都采用默认行为，即判断容器返回是否为0来判断是否探测成功 2.3次探测失败后，Liveness重启容器，Readiness将容器设置不可用，不接收Service转发的请求 3.两者独立互不依赖，可同时使用，Liveness探测判断容器是否需要重启以实现自愈，Readiness判断容器是否已经准备好对外提供服务。 By Dazzle Lin            updated 2019-07-02 22:23:45 "},"Chapter4/数据管理.html":{"url":"Chapter4/数据管理.html","title":"数据管理","keywords":"","body":"数据管理 Volume 容器和Pod生命周期很短，容器销毁时，保存在容器内部文件系统中的数据都会被清除 Volume生命周期独立于容器 本质只是一个目录，当Volume被mount（安置）到Pod,Pod上所有容器都可以访问Volume K8S Volume也支持多种backend(后台)类型 emptyDir 最基本的Volume类型，Host的空目录 emptyDir Volume生命周期与Pod一致，对于容器来说持久 模拟producer-consumer场景。Pod有两个容器producer和consumer，共享一个volume。producer负责往Volume写数据，consumer从volume读数据 apiVersion: v1 kind: Pod metadata: name: producer-consumer spec: containers: - image: busybox name: producer volumeMounts: # producer容器将volume Mount 到/producer_dir - mountPath: /producer_dir name: shared-volume args: - /bin/sh - -c #将数据写入hello - echo \"hello world\" > /producer_dir/hello ; sleep 30000 - image: busybox name: consumer volumeMounts: # consumer容器将volume Mount 到/consumer_dir - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c #cat从文件读取 - cat /consumer_dir/hello ; sleep 30000 volumes: #定义一个emptyDir类型的Volume shared-volume - name: shared-volume emptyDir: {} [docker@k8s-master ~]$ vi emptyDir.yml [docker@k8s-master ~]$ kubectl apply -f emptyDir.yml pod/producer-consumer created [docker@k8s-master ~]$ kubectl get pod NAME READY STATUS RESTARTS AGE producer-consumer 2/2 Running 0 32s #kubectl logs显示容器consumer成功读到了producer写入的数据，验证了两个容器共享volume [docker@k8s-master ~]$ kubectl logs producer-consumer consumer hello world 因为emptyDir是Docker Host文件系统里的目录，效果等同执行docker run -v /producer_dir和docker run -v /consumer_dir. 去对应Node节点通过docker inspect查看容器详细配置 docker ps docker inspect containerId emptyDir是Host创建的临时目录 优点 能够方便地为Pod中的容器提供共享存储，不需要额外配置，不具备持久性。Pod不存在，emptyDir也旧没有了 适合场景 Pod中容器需要临时共享存储空间 hostPath 将Docker Host文件系统中已存在的目录mount给Pod容器。 大多数应用不会使用，实际上增加了Pod和节点的耦合 适用 需要访问K8S和docker内部数据的应用可以使用 比如kube-apiserver和kube-controller-manager 查看kube-apiserver Pod的配置 kubectl edit --namespace=kube-system pod kube-apiserver-k8s-master Pod销毁，hostPath对用目录会保留 外部Storage Provider 略 PersistentVolume & PersistentVolumeClaim PV是外部存储系统中的一块存储空间，管理员维护和创建，具有持久性，生命周期独立于Pod PVC是对PV的申请，普通用户创建和维护，需要为Pod分配存储资源时，用户可以创建一个PVC，指明存储资源的容量大小和访问模式，K8S会自动查找并提供相应的pv #验证PV是否可用 kubectl exec mypod1 touch /mydata/hello ls /nfsdata/pv1/ 在Pod创建的文件/mydata/hello是否保存到NFS服务器目录/nfsdata/pv1/中 回收 kubectl delete pvc kubectl delete pv 当PVC mypvc1被删除后，K8S启动一个新的Pod recycler-for-mypv1（跟设置的回收策略相关）,这个Pod用来清除PV mypv1的数据，初始状态为Released,表示已经接触了Bound，清除完成后状态变为可用 PV动态供给 提前创建PV，通过PVC进行申请的PV叫静态供给 如果没找到满足PVC条件的PV，动态创建PV的叫动态供给。 动态供给通过StorageClass实现 StorageClass支持Delete和Retain两种reclaimPolicy,默认Delete Secret & Configmap 应用启动中难免有敏感字符，K8S利用Secret以密文方式存储数据，以Volume形式被mount到Pod，容器通过文件方式使用Secret中得敏感数据，此外，容器也可以通过环境变量方式使用数据 Secret通过命令行或YAML创建。 创建Secret 通过--from-literal kubectl create secret generic mysecret --from-literal=username=admin --from-literal=password=123456 每个--from-literal对应一个信息条目 通过--from-file echo -n admin > ./username echo -n 123456 > ./password kubectl create secret generic mysecret --from-file=./username --from-filr=./password 通过--from-env-file cat env.txt username=admin password=123456 EOF kubectl create secret generic mysecret --from-env-file=env.txt 文件env.txt每行key=value对应一个信息条目 通过YAML配置文件 apiVersion: v1 kind: Secret metadata: name: mysecret data: username: YWRtaW4= password: MTIzNDU2 文件中得敏感数据必须是通过BASE64编码后的结果 echo -n admin | base64 echo -n 123456 | base64 [docker@k8s-master ~]$ kubectl apply -f mysecret.yml secret/mysecret created 查看Secret [docker@k8s-master ~]$ kubectl get secret mysecret NAME TYPE DATA AGE mysecret Opaque 2 57s 显示有两个数据条目，kubectl describe secret查看条目得Key 查看Value kubectl edit secret mysecret #base64反编译 echo -n YWRtaW4= | base64 --decode 在Pod中使用Secret pod可以通过Volume或者环境变量方式使用Secret Volume方式 apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 30000 volumeMounts: #将foo mount到容器路径/etc/foo，指定读写权限只读 - mountPath: \"/etc/foo\" name: foo readOnly: true #定义volume foo，来源secret mysecret volumes: - name: foo secret: secretName: mysecret [docker@k8s-master ~]$ kubectl apply -f mypod.yml pod/mypod created [docker@k8s-master ~]$ kubectl exec -it mypod sh / # ls /etc/foo password username / # cat /etc/foo/username admin/ # / # cat /etc/foo/password 123456/ # K8S会在指定得路径/etc/foo下为每条敏感数据创建一个文件，Key为文件名，Value以铭文存放在文件中。 可以自定义文件名 apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 30000 volumeMounts: #将foo mount到容器路径/etc/foo，指定读写权限只读 - mountPath: \"/etc/foo\" name: foo readOnly: true #定义volume foo，来源secret mysecret volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-username - key: password path: my-group/my-password 以Volume方式使用的Secret支持动态更新：Secret更新后，容器中的数据也会更新 环境变量方式 apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: busybox args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 30000 volumeMounts: #将foo mount到容器路径/etc/foo，指定读写权限只读 - mountPath: \"/etc/foo\" name: foo readOnly: true env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password [docker@k8s-master ~]$ kubectl apply -f mypod.yml pod/mypod created [docker@k8s-master ~]$ kubectl exec -it mypod sh / # echo $SECRET_USERNAME admin / # echo $SECRET_PASSWORD 123456 无法支撑Secret动态更新 By Dazzle Lin            updated 2019-07-02 22:25:25 "},"Chapter5/Helm-k8s的包管理工具.html":{"url":"Chapter5/Helm-k8s的包管理工具.html","title":"Helm-k8s的包管理工具","keywords":"","body":"Helm-k8s的包管理工具 KS8能够很好的组织和编排容器，但它缺少一个更高层次得应用打包工具，Helm就是干这件事 Mysql服务 1.Service,让外界能够访问到MYSQL apiversion: v1 kind: Service metadata: name: my-mysql labels: app: my-mysql spec: ports: - name: mysql port: 3306 targetPort: mysql selector: app: my-mysql 2.secret,定义mysql密码 apiversion: v1 kind: Secret metadata: name: my-mysql labels: app: my-mysql type: Opaque data: mysql-root-password: \"\" mysql-password: \"\" 3.PersistentVolumeClaim为mysql申请持久化存储空间 kind: PersistentVolumeClaim apiversion: v1 metadata: name: my-mysql labels: app: my-mysql spec: accessModes: - \"ReadWriteOnce\" resources: request: storage: \"8Gi\" 4.Deployment,部署mysql Pod apiversion: extensions/v1beta1 kind: Deployment metadata: name: my-mysql labels: app: my-mysql spec: template: metadata: labels: app: my-mysql spec: containers: - name: my-mysql image: \"mysql:5.7.14\" env: - name: MYSQL_ROOT_PASSWORD volueFrom: secretKeyRef: name: my-mysql key: mysql-root-password - name: MYSQL_PASSWORD volueFrom: secretKeyRef: name: my-mysql key: mysql-password - name: MYSQL_USER value: \"\" - name: MYSQL_DATABASE value: “” ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql volumes: - name: data persistentVolumeClaim: claimName: my-mysql Helm架构 chart 创建一个应用得信息集合，包括各种K8S对象得配置模板、参数定义、依赖关系、文档说明等。chart是应用部署得自包含逻辑单元。可以想象成apt、yum中得软件安装包 release chart的运行实例，代表一个正在运行的应用。当chart被安装到k8s集群，就生成一个release。chart能够多次安装到同一个集群，每次安装都是一个release helm能够做什么 1.从零创建chart 2.与存储chart的仓库交互，拉取、保存和更新chart 3.在k8s集群中安装和卸载release 4.更新、回滚和测试release Helm包含组件： Helm客户端（管理chart） 终端用户使用的命令行工具 1.在本地开发chart 2.管理chart仓库 3.与Tiller服务器交互 4.在远程K8S集群安装chart 5.查看release信息 6.升级或卸载已有release Tiller服务器（管理release） 运行在k8s集群上，会处理Helm客户端请求，与K8S APIserver交互 1.监听来自Helm客户端请求 2.通过chart构建release 3.在K8S中安装chart,并跟踪release状态 4.通过APIserver升级或卸载已有release 安装Helm 将Helm客户端安装在能执行kubectl命令的节点上。 [docker@k8s-master ~]$ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 7028 100 7028 0 0 6316 0 0:00:01 0:00:01 --:--:-- 6320 Helm v2.14.0 is already latest Run 'helm init' to configure helm. [docker@k8s-master ~]$ helm version Client: &version.Version{SemVer:\"v2.14.0\", GitCommit:\"05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0\", GitTreeState:\"clean\"} Error: could not find tiller wget https://storage.googleapis.com/kubernetes-helm/helm-v2.14.0-linux-amd64.tar.gz tar xf helm-v2.9.1-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin rm -rf linux-amd64# 查看版本，不显示出server版本，因为还没有安装serverhelm version 安装helm的bash命令补全脚本 helm completion bash > .helmrc echo \"source .helmrc\" >> .bashrc 重新登录 可以Tab补全了 Tiller服务器 #删除Tiller kubectl get all --all-namespaces | grep tiller kubectl get all -n kube-system -l app=helm -o name|xargs kubectl delete -n kube-system #提前拉去镜像gcr.io/kubernetes-helm/tiller:v2.14.0 docker pull sapcc/tiller:v2.14.0 docker tag sapcc/tiller:v2.14.0 gcr.io/kubernetes-helm/tiller:v2.14.0 #安装tiller helm init kubectl get --namespace=kube-system pod kubectl describe pod tiller-deploy-f8c677c4-68v78 -n kube-system #如果仍然因为网络拉取不下来 kubectl edit deploy tiller-deploy -n kube-system 将 image gcr.io/kubernetes-helm/tiller:v2.14.0 替换成 image: sapcc/tiller:v2.14.0 #保存后，kubernetes会自动生效，再次查看pod，已经处于running状态了 kubectl get pod -n kube-system 成功标志 [docker@k8s-master ~]$ helm version Client: &version.Version{SemVer:\"v2.14.0\", GitCommit:\"05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.14.0\", GitCommit:\"05811b84a3f93603dd6c2fcfe57944dfa7ab7fd0\", GitTreeState:\"clean\"} Tiller本身作为容器化应用运行在k8s cluster中 kubectl get --namespace=kube-system deployment kubectl get --namespace=kube-system svc（service） kubectl get --namespace=kube-system pod #查看pod错误 kubectl describe pod tiller-deploy-765dcb8745-76vqm -n kube-system gcr.io/kubernetes-helm/tiller:v2.14.0 使用Helm #查看当前可安装的chart helm search helm repo list #添加仓库 helm repo add #安装chart helm install 软件name（stable/mysql） 安装如果权限不足，添加权限 [docker@k8s-master ~]$ helm install stable/mysql Error: no available release name found [docker@k8s-master ~]$ 添加权限 kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p'{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' 解析 [docker@k8s-master ~]$ helm install stable/mysql #chart本次部署的描述信息 #release的名字，没用-n，因此Helm随机生成一个 NAME: agile-sheep LAST DEPLOYED: Tue May 28 21:27:22 2019 #release部署的namespace，默认default，也可以通过--namespace指定 NAMESPACE: default #DEPLOYED，表示已经将chart部署到集群 STATUS: DEPLOYED #当前release包含的资源，命名格式为ReleaseName-ChartName RESOURCES: ==> v1/PersistentVolumeClaim NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE agile-sheep-mysql Pending 0s ==> v1/Pod(related) NAME READY STATUS RESTARTS AGE agile-sheep-mysql-d9787fcd4-89ktn 0/1 Pending 0 0s ==> v1/Secret NAME TYPE DATA AGE agile-sheep-mysql Opaque 2 0s ==> v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE agile-sheep-mysql ClusterIP 10.104.230.48 3306/TCP 0s ==> v1beta1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE agile-sheep-mysql 0/1 1 0 0s #release的使用方法 NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: agile-sheep-mysql.default.svc.cluster.local To get your root password run: MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default agile-sheep-mysql -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode; echo) To connect to your database: 1. Run an Ubuntu pod that you can use as a client: kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il 2. Install the mysql client: $ apt-get update && apt-get install mysql-client -y 3. Connect using the mysql cli, then provide your password: $ mysql -h agile-sheep-mysql -p To connect to your database directly from outside the K8s cluster: MYSQL_HOST=127.0.0.1 MYSQL_PORT=3306 # Execute the following commands to route the connection: export POD_NAME=$(kubectl get pods --namespace default -l \"app=agile-sheep-mysql\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl port-forward $POD_NAME 3306:3306 mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD} 通过kubectl get查看release各个对象 [docker@k8s-master ~]$ kubectl get service agile-sheep-mysql NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE agile-sheep-mysql ClusterIP 10.104.230.48 3306/TCP 5m4s [docker@k8s-master ~]$ kubectl get deployment agile-sheep-mysql NAME READY UP-TO-DATE AVAILABLE AGE agile-sheep-mysql 0/1 1 0 5m18s [docker@k8s-master ~]$ kubectl get pvc agile-sheep-mysql NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE agile-sheep-mysql Pending 5m46s 未准备PersistentVolume，因此当前release不可用 #helm list显示已经部署的release [docker@k8s-master ~]$ helm list #helm delete可以删除release [docker@k8s-master ~]$ helm delete agile-sheep chart chart是Helm的应用打包格式，由一系列文件组成。可以很简单只部署一个服务，也可以很负责部署一个应用 chart将文件放置在预定义的目录结构中，通常整个chart被打成tar包，而且标注版本信息，便于Helm部署 #一旦安装了chart，即可查看tar包 [docker@k8s-master ~]$ ls ~/.helm/cache/archive/ mysql-0.3.5.tgz 目录名就是chart名字（不带版本信息） 1.Chart.yaml:描述chart概要，name和version必填 2.README.md:相当于chart使用文档 3.LICENSE:文本文件，描述chart的许可信息 4.requirements.yaml:通过该文件指定chart依赖的其他chart,安装过程中依赖的chart也被一起安装 5.values.yaml:chart支持在安装时根据参数进行定制化配置，而该文件提供 了配置参数的默认值 6.templates目录：各类K8S资源的配置模板都放置在这里，Helm会将values.yaml中的参数值注入模板中，生成标准的YAML配置文件 模板时chart最重要的部分，也是Helm最强大的部分。模板增加了应用部署灵活性 7.templates/NOTES.txt:chart的简易使用文档，chart安装成功后显示此文档内容 可以在该文件中插入配置参数，Helm会动态注入参数值 chart模板 Go语言模板编写chart {{template \"mysql.fullname\"}} 定义Secret的name。template 作用引入一个子模版。子模版在templates/_helpers.tp1文件中定义 注意：如果存在一些信息多个模板使用，则可在templates/_helpers.tp1中定义子模版，然后通过templates函数引用 chart和release是Helm预定义的对象，每个对象有自己的属性 略 #查看chart的values.yaml内容 helm inspect values stable/mysql [docker@k8s-master ~]$ helm inspect values stable/mysql ## mysql image version ## ref: https://hub.docker.com/r/library/mysql/tags/ ## image: \"mysql\" imageTag: \"5.7.14\" ## Specify password for root user ## ## Default: random 10 character string # mysqlRootPassword: testing ## Create a database user ## # mysqlUser: # mysqlPassword: ## Allow unauthenticated access, uncomment to enable ## # mysqlAllowEmptyPassword: true ## Create a database ## # mysqlDatabase: ## Specify an imagePullPolicy (Required) ## It's recommended to change this to 'Always' if the image tag is 'latest' ## ref: http://kubernetes.io/docs/user-guide/images/#updating-images ## imagePullPolicy: IfNotPresent livenessProbe: initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 3 readinessProbe: initialDelaySeconds: 5 periodSeconds: 10 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 3 ## Persist data to a persistent volume persistence: enabled: true ## database data Persistent Volume Storage Class ## If defined, storageClassName: ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## # storageClass: \"-\" accessMode: ReadWriteOnce size: 8Gi ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: requests: memory: 256Mi cpu: 100m # Custom mysql configuration files used to override default mysql settings configurationFiles: # mysql.cnf: |- # [mysqld] # skip-name-resolve ## Configure the service ## ref: http://kubernetes.io/docs/user-guide/services/ service: ## Specify a service type ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types type: ClusterIP port: 3306 # nodePort: 32000 事先创建好相应PV->mysql-pv.yml apiversion: v1 kind: PersistentVolume metadata: name: mysql-pv spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi persistentVolumeReclaimPolicy: Retain nfs: path: /nfsdata/mysql-pv server: 192.168.56.105 kubectl apply -f mysql-pv.yml kubectl get pv 定制化安装chart Helm有两种方式传递配置参数 1.指定自己的values文件。通常做法 首先通过helm inspect values mysql > myvalues.yaml生成values文件 设置mysqlRootPassword 最后执行helm install --values=mysqlvalues.yaml mysql 2.通过--set直接传入参数 #-n即设置release为my,各类资源名称即为my-mysql helm install stable/mysql --set mysqlRootPassword=abc123 -n my #通过helm list和helm status my查看chart最新状态 升级和回滚release 通过--values或--set应用新的配置。升级mysql到5.7.15 helm upgrade --set imageTag=5.7.15 mystable/mysql kubectl get deployment my-mysql -o wide #查看release所有版本 helm history my #回滚到任何版本 helm rollback my 1 开发自己的chart 创建chart #helm会帮我们创建目录，并生成各类chart文件 helm create mychart 新建的chart默认包含一个nginx应用示例 调试chart Helm提供了debug工具 helm lint:检测chart语法，报告错误以及给出建议 helm lint mychart helm install --dry-run --debug：模拟安装安装chart，并输出每个模板生成的YAML内容 helm install --dry-run mychart --debug 安装chart Helm支持四种安装方法 1.安装仓库中的chart helm install stable/nginx 2.通过tar包安装 helm install ./nginx-1.2.3.tgz 3.通过chart本地目录安装 helm install ./nginx 4.通过URL安装 helm install https://example.com/charts/nginx-1.2.3.tgz 将chart添加到仓库 任何HTTP Server都可以用作chart仓库 在k8s-node1上搭建仓库 1.在node1上启动httpd容器 mkdir /var/www docker run -d -p 8080:80 -v /var/www/:/usr/local/apache2/htdocs/ httpd 2.helm package将mychart打包 helm package mychart 3.生成仓库的Index文件 mkdir myrepo mv mychart-0.1.0.tgz myrepo/ helm repo index myrepo/ --url http://node1IP:8080/charts ls myrepo/ Helm会扫描myrepo目录中的所有tgz包并生成index.yaml --url指定的是新仓库的访问路径。 新生成的index.yaml记录了当前仓库下所有chart的信息 4.将mychart-0.1.0.tgz和index.yaml上传到k8s-node1的/var/www/charts目录 5.通过helm repo add将新仓库添加到Helm helm repo add newrepo http://node1IP:8080/charts helm repo list 6.除了newrepo/mychart还有一个local/mychart，在执行2打包时mychart也被同步到local的仓库 helm search mychart 7.直接从新仓库安装mychart helm install newrepo/mychart 8.若以后添加了新的chart,可通过helm repo update更新本地的index.相当于apt-get update helm repo update By Dazzle Lin            updated 2019-07-03 12:13:38 "},"Chapter6/网络.html":{"url":"Chapter6/网络.html","title":"网络","keywords":"","body":"网络 K8S采用的基于扁平地址空间的网络模型，集群中的每个Pod都有自己的IP地址，Pod之间不需要配置NAT就能直接通信。 同一个Pod中的容器共享Pod的IP，能够通过localhost通信。 K8S网络模型 Pod内不同容器之间的通信 Pod被调度到某个节点，Pod中的所有容器都在这个节点运行。 容器共享相同本地文件系统、网络命名空间和IPC 不同Pod之间不存在端口冲突，因为Pod都有自己的IP Pod之间的通信 Pod得IP是集群可见，即集群中的任何其他Pod和节点都可以通过IP直接与Pod通信。 Pod内部和外部使用的是同一个IP Pod与Service的通信 Pod间直接通过IP地址通信，但前提是直到对方IP。 Service提供了访问Pod的抽象层，无论后端Pod如何变化，Service都作为稳定的前端对外提供服务。 Service还提供了高可用和负载均衡功能。负责将请求转发给正确的Pod 外部访问 无论是Pod还是Service对于K8S之外的世界来说都是私有的IP K8s提供了两种方式让外界能够与Pod通信 NodePort Service通过Cluster节点的静态端口对外提供服务，外部可以通过:访问Service LoadBalancer Service利用cloud provider提供的load balancer对外提供服务，cloud provider负责将load balancer的流量导向Service。目前支持的cloud provider有GCP、AWS、Azur等。 网络方案 K8S采用了Container Networking Interface(CNI）规范 CNI是由CentOs提出的容器网络规范，使用插件（Plugin）模型创建容器的网络栈。 Network Policy Network Policy通过Label选择Pod,并指定其他Pod或外界如何与这些Pod通信 当为Pod定义了Network Policy时，只有Policy允许的流量才能访问Pod 不是所有的网络方案都支持，比如Flanne不支持 部署Canal Canal:用Flannel实现K8S集群网络，用Calico实现Network Policy 没有太好的办法直接切换使用不同的网络方案，基本上只能重建集群 kubeadm init --apiserver-advertise-address 192.168.129.132 --pod-network-cidr=10.244.0.0/16 查看文档安装网络 kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml 部署之后查看Canal kubectl get --namespace=kube-system daemonset canal kubectl get --namespace=kube-system pod -o wide|grep canal 注意： 会出现拉取镜像失败，提前拉取修改tag 实践Network Policy 部署一个httpd应用，配置文件httpd.yaml httpd有三个副本，通过NodePort类型的Service对外提供服务 apiVersion: apps/v1beta1 kind: Deployment metadata: name: httpd spec: replicas: 3 template: metadata: labels: run: httpd spec: containers: - name: httpd image: httpd:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: httpd-svc spec: type: NodePort selector: run: httpd ports: - protocol: TCP nodePort: 30000 port: 8080 targetPort: 80 kubectl apply -f httpd.yaml kubectl get pod -o wide kubectl get service httpd-svc 当前没定义Network Policy，验证应用可以被访问 1.启动一个Busybox Pod，既可以访问Service,也可以Ping到副本Pod kubectl run busybox --rm -ti --image=busybox /bin/sh wget httpd-svc:8080 ping PodIP 2.集群节点既可以访问Service,也可以Ping到副本Pod curl IP:8080 ping -c 3 PodIP 3.集群外可以访问Service curl IP:30000 创建Network Policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: access-httpd spec: podSelector: matchLabels: #定义将此Network Policy中的访问规则应用于label为run:httpd的pod，即httpd应用的三个副本Pod run: httpd ingress: - from: - podSelector: matchLabels: #ingress中定义只有label为access:\"true\"的Pod才能访问 access: \"true\" ports: - protocol: TCP #只能访问80端口 port: 80 kubectl apply -f policy.yaml kubectl get networkpolicy 1.busybox Pod已经不能访问Service kubectl run busybox --rm -ti --image=busybox /bin/sh wget httpd-svc:8080 timeout 如果Pod添加了label access:\"true\"就能访问到应用 kubectl run busybox --rm -ti --labels=\"access=true\" --image=busybox /bin/sh 2.集群节点和集群外都已经无法访问Service 希望能够访问，需要修改如下 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: access-httpd spec: podSelector: matchLabels: #定义将此Network Policy中的访问规则应用于label为run:httpd的pod，即httpd应用的三个副本Pod run: httpd ingress: - from: - podSelector: matchLabels: #ingress中定义只有label为access:\"true\"的Pod才能访问 access: \"true\" - ipBlock: cidr: 集群主机网关（192.168.129.132/24） ports: - protocol: TCP #只能访问80端口 port: 80 测试集群节点和外网访问 By Dazzle Lin            updated 2019-07-02 22:27:19 "},"Chapter7/Kubernetes Dashboard.html":{"url":"Chapter7/Kubernetes Dashboard.html","title":"Kubernetes Dashboard","keywords":"","body":"Kubernetes Dashboard 安装 wget http://pencil-file.oss-cn-hangzhou.aliyuncs.com/blog/kubernetes-dashboard.yaml 添加 # ------------------- Dashboard Service ------------------- # kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: NodePort #新增 ports: - port: 443 targetPort: 8443 selector: 修改 --- # ------------------- Dashboard Deployment ------------------- # 。。。。。 image: siriuszg/kubernetes-dashboard-amd64 部署 kubectl create -f kubernetes-dashboard.yaml 查看deployment和Service [docker@k8s-master ~]$ kubectl get --namespace=kube-system deployment kubernetes-dashboard NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-dashboard 1/1 1 1 2m53s [docker@k8s-master ~]$ kubectl get --namespace=kube-system service kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.104.164.187 443:30129/TCP 4m16s 访问https://:/ 可能会出现证书过期，百度解决 配置登录权限(待测) dashboard-admin.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --------------------- 作者：终南山道人 来源：CSDN 原文：https://blog.csdn.net/qq_24513043/article/details/82460759 版权声明：本文为博主原创文章，转载请附上博文链接！ apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system kubectl apply -f dashboard-admin.yaml 查看token kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') By Dazzle Lin            updated 2019-07-02 22:28:16 "},"Chapter8/Kubernetes集群监控.html":{"url":"Chapter8/Kubernetes集群监控.html","title":"Kubernetes集群监控","keywords":"","body":"Kubernetes集群监控 Weave Scope Heapster K8S原生得集群监控方案 部署 git clone https://github.com/kubernetes-retired/heapster.git kubectl apply -f heapster/deploy/kube-config/influxdb/ kubectl apply -f heapster/deploy/kube-config/rbac/heapster-rbac.yaml kubectl get --namespace=kube-system deployment | grep -e heapster -e monitor kubectl get --namespace=kube-system service | grep -e heapster -e monitor kubectl get --namespace=kube-system pod -o wide | grep -e heapster -e monitor #修改monitoring-grafana为NodePort kubectl edit --namespace=kube-system service monitoring-grafana 访问http://192.168.129.132:30209/ Prometheus Operator By Dazzle Lin            updated 2019-07-02 22:29:03 "},"Chapter9/Kubernetes集群日志管理.html":{"url":"Chapter9/Kubernetes集群日志管理.html","title":"Kubernetes集群日志管理","keywords":"","body":"Kubernetes集群日志管理 By Dazzle Lin            updated 2019-07-02 22:04:26 "},"Chapter10/K8S常用命令.html":{"url":"Chapter10/K8S常用命令.html","title":"K8S常用命令","keywords":"","body":"K8S常用命令 #部署应用 kubectl apply -f httpd.yml #了解信息 kubectl get deployment httpd -o wide #了解更详细信息 kubectl describe deployment #查看副本信息 kubectl get replicaset -o wide kubectl describe replicaset #查看副本Pod kubectl get pod kubectl describe pod By Dazzle Lin            updated 2019-07-02 22:29:44 "},"Chapter11/apply部署过程.html":{"url":"Chapter11/apply部署过程.html","title":"apply部署过程","keywords":"","body":"apply部署过程 1.创建Deployment httpd 2.创建ReplicaSet httpd-..... 3.创建三个Pod 4.当前镜像为 。。。。 By Dazzle Lin            updated 2019-07-02 22:29:55 "}}